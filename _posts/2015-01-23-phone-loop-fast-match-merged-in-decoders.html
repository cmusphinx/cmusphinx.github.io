---
layout: post
status: publish
published: true
title: Phone Loop Fast-Match Merged in Decoders
author:
  display_name: admin
  login: admin
  email: nshmyrev@nexiwave.com
  url: ''
author_login: admin
author_email: nshmyrev@nexiwave.com
date: '2015-01-23 15:58:14 +0100'
date_gmt: '2015-01-23 12:58:14 +0100'
---
<p>These days speech recognition technology is under focus of many large corporations like Google, Baidu, Microsoft. It is an exciting time of shifting paradigms and approaches which lead to quite significant improvement of the accuracy and stability of the technology. Many ideas which seemed fundamental are now questioned, for example, the modern research tries to replace the Hidden Markov Models with recursive neural networks and more complex structures like long-short term memory (LTSM) networks. There is a lot of marketing here, for example LTSM papers often present that such networks improve frame classification accuracy significantly which was never a parameter to optimize in ASR system because the word-error rate results of LTSM networks are not as great as frame accuracy results.</p>
<p>One idea that was quite successfully challenged is the use of feature context in speech recognition system. MFCC with delta-deltas usually use just 7-9 frames around the current frame while modern deep neural network (DNN) classifiers can use up to 30 frames successfully. That certainly improves the accuracy of speech recognition for DNN system. One important thing here is that we still use breadth-first search in our decoders, thus our context is only used in classification, not in search. This is a major drawback in modern systems.</p>
<p><a id="more"></a><a id="more-961"></a></p>
<p>If we consider graph best path search there are two approaches here - breadth first search where we first explore all possible branches coming out of a node and depth-first approach where we continue to explore the path till some point without looking on branches. Modern decoders mostly use breadth-first search (BFS), in many cases it is considered a suboptimal approach. </p>
<p>For example, if we have some noise in a frame and next frame is correct we can not recover from it because we already pruned the correct path on the noisy frame. There is no way to recover. Only by looking on several frames at once we can figure out which path is correct and which is not. </p>
<p>Next advantage of depth-first search is speed. By using larger context we can quickly reduce the hypothesis space, for example, by looking on 3-4 following phonemes we can reduce the amount of words to search.</p>
<p>Depth-first search was quite popular before in 90-es, many decoders those days were using it like <a href="http://dl.acm.org/citation.cfm?doid=1075527.1075650">Dragon</a> decoder or <a href="http://www.isca-speech.org/archive/icslp_2000/i00_4334.html">IBM Stack decoder</a> or <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.2254">Noway decoder</a>. Unfortunately, with introduction of WFST framework, those decoders declined.</p>
<p>WFST framework is also an interesting case to consider here. The problem with triphone models is that they explode when we consider cross-word transition. We do not know following phone so we have to consider all possible variants and every possible history, that grows the search space significantly. Before this problem was solved in a different ways, for example, developers use multipass search without cross-word triphones first and then rescored with cross-word triphones (pocketsphinx approach). This problem is well considered in <a href="http://libra.msra.cn/Publication/50835054/dynamic-network-decoding-revisited">Dynamic Network Decoding Revisited</a> paper by H. Soltau, G. Saon. </p>
<p>By using graph reduction operation we solve the very specific problem - improve speed of decoding by properly compressing cross-word contexts. With free implementation in OpenFST toolkit this method become very popular. This is an improvement in decoding speed which allowed to improve decoding performance two-three folds, but it also has disadvantages. Due to the strict and simplistic formalism of WFST it is not easy to use it to perform more complex searches and integrate more complex models into search, for example, hierarchical language model. Also, it is quite memory extensive because precompiled WFST graph has to reside in system memory during decoding.  An attempts to overcome WFST restrictions continue these days, one possible approach is <a href="https://www-i6.informatik.rwth-aachen.de/publications/download/722/RybachDavidSchl%7Bu%7DterRalfNeyHermann--AComparativeAnalysisofDynamicNetworkDecoding--2011.pdf">dynamic context compression</a> in decoder inspired by WFST ideas, which still requires recombination of word labels and careful tracking of context. And, if we consider simple things like noises and paralinguistic fillers, we make our system much more complex to implement in WFST framework.</p>
<p>If we get out of restrictions of breadth-first search we get another solution here. Just by looking on following phones we can greatly reduce amount of hypothesis to consider during cross-word transition and thus we do not need complex WFST compression anymore. And here we just need to tolerate a bit the delay in recognition. The allowed delay to tolerate can be derived from human response expectation. About 0.2 to 0.5 seconds is ok, thus we can consider up to 50 frames ahead. This is an old idea which was used in 90-es and somewhat supported in pocketsphinx: a phonetic loop fast match. We first decode the audio with very fast phonetic decoder and only after a delay we start main large vocabulary search.  This way we not only improve word transition search space but also improve decoding inside lextree because we can predict following phones efficiently. Because of that we consider phone loop search as quite important feature of the decoder. Such ideas has been long advocated by Dr. James Baker and by Dr. Tony Robinson from <a href="http://www.cantabresearch.com">CantabResearch</a>.</p>
<p>Recently we improved fast match in sphinx4 and fixed fast-match issues in pocketsphinx. Since today we include phone-loop fast match both in sphinx4 and pocketsphinx by default, it greatly improves the speed of decoding and combined with PTM models you can decode large vocabulary speech on desktop in realtime with sphinx4 and high accuracy. That's pretty big improvement. Please <a href="http://github.com/cmusphinx">checkout either pocketsphinx or sphinx4</a>, try it out and let us know what do you think.</p>
